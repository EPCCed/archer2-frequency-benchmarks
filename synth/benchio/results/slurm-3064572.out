#!/bin/bash

# Slurm job options (name, compute nodes, job time)
#SBATCH --job-name=benchio
#SBATCH --time=00:20:00
#SBATCH --nodes=1
#SBATCH --tasks-per-node=128
#SBATCH --cpus-per-task=1
# Replace [budget code] below with your budget code (e.g. t01)
#SBATCH --account=z19
#SBATCH --partition=standard
#SBATCH --qos=short

cat $0

module swap craype-network-ofi craype-network-ucx
module swap cray-mpich cray-mpich-ucx

module use /work/z19/shared/sfarr/modulefiles
module load adios/2.8.3

module load cray-hdf5-parallel
module load cray-netcdf-hdf5parallel

export OMP_NUM_THREADS=1

ulimit -s unlimited

# Launch the parallel job

export FI_OFI_RXM_SAR_LIMIT=64K
export MPICH_MPIIO_HINTS=*:cray_cb_write_lock_mode=0,*:cray_cb_nodes_multiplier=1

srun --unbuffered --distribution=block:block --hint=nomultithread \
	/work/z19/z19/shared/ARCHER2_CSF/benchio/benchio/benchio 2048 2048 2048 global mpiio fullstriped

srun --unbuffered --distribution=block:block --hint=nomultithread \
        /work/z19/z19/shared/ARCHER2_CSF/benchio/benchio/benchio 2048 2048 2048 global adios unstriped

sleep 120
echo "\n++Energy use:"
sacct -j $SLURM_JOB_ID --format="JobID,NNodes,NTasks,ReqCPUFreq,ElapsedRaw,ConsumedEnergyRaw"

cd ../16_nodes
sbatch 1_sub.slurm

Inactive Modules:
  1) cray-mpich


Lmod is automatically replacing "cce/11.0.4" with "gcc/10.2.0".


Lmod is automatically replacing "PrgEnv-cray/8.0.0" with "PrgEnv-gnu/8.0.0".


Lmod is automatically replacing "craype-network-ucx" with "craype-network-ofi".


Inactive Modules:
  1) cray-mpich-ucx


 Simple Parallel IO benchmark
 ----------------------------

 Running on          128  process(es)

 Running on            1  node
 Node number           0  is nid003765 with          128  processes

 Process grid is (           4 ,            4 ,            8 )
 Array size is   (         512 ,          512 ,          256 )
 Global size is  (        2048 ,         2048 ,         2048 )

 Total amount of data =    64.000000000000000       GiB

 Clock resolution is    1.0000000000000000E-003 , usecs

 Using the following IO methods
 ------------------------------
 mpiio                                                           

 Using the following stripings
 -----------------------------
 fullstriped                                                     


 ------
 MPI-IO                                                          
 ------

 Writing to fullstriped/mpiio.dat                                           
 time =    60.040098140016198      , rate =    1.0659542869291974       GiB/s

 --------
 Finished
 --------


 Simple Parallel IO benchmark
 ----------------------------

 Running on          128  process(es)

 Running on            1  node
 Node number           0  is nid003765 with          128  processes

 Process grid is (           4 ,            4 ,            8 )
 Array size is   (         512 ,          512 ,          256 )
 Global size is  (        2048 ,         2048 ,         2048 )

 Total amount of data =    64.000000000000000       GiB

 Clock resolution is    1.0000000000000000E-003 , usecs

 Using the following IO methods
 ------------------------------
 adios                                                           

 Using the following stripings
 -----------------------------
 unstriped                                                       


 ------
 Adios2                                                          
 ------

 Writing to unstriped/adios.dat                                             
 time =    18.995858475565910      , rate =    3.3691554441891767       GiB/s

 --------
 Finished
 --------

\n++Energy use:
JobID          NNodes   NTasks ReqCPUFreq ElapsedRaw ConsumedEnergyRaw 
------------ -------- -------- ---------- ---------- ----------------- 
3064572             1             Unknown        244                   
3064572.bat+        1        1          0        244                 0 
3064572.ext+        1        1          0        244                 0 
3064572.0           1      128         2G         85             28412 
3064572.1           1      128         2G         29              8896 
Submitted batch job 3064585
